{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjaqnJ88lejB"
   },
   "source": [
    "# Problem 2: Higher-order visual representations and DNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766091513267,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "2wL1gDr9lejF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import torchvision.transforms as transforms\n",
    "# if you get an error re: torchvision, run this line first: \n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OE2CMK8lejG"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "First we define some functions - you can skip down to the `Load Data` Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1766091513328,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "_8nzbv_YlejG"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def visualize_rdm(rdm, ax=None, title=None, vmax=1.75, cmap='viridis'):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    im = ax.imshow(rdm,  vmin=0, vmax=vmax, cmap=cmap)\n",
    "    for i_cat in range(0, 49, 7):\n",
    "      ax.plot([i_cat, i_cat], [0, 49], 'k')\n",
    "      ax.plot([0, 49], [i_cat, i_cat], 'k')\n",
    "    ax.set(xticks = np.arange(4, 49, 7))\n",
    "    ax.set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
    "    ax.set(yticks = np.arange(4, 49, 7))\n",
    "    ax.set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
    "    ax.set(xlim = [0, 49], ylim = [49, 0])\n",
    "\n",
    "    plt.colorbar(im, ax=ax, label='Dissimilarity')\n",
    "\n",
    "    if title is not None:\n",
    "        ax.set(title=title)\n",
    "    return im\n",
    "\n",
    "\n",
    "def compute_RDM(resp):\n",
    "  \"\"\"Compute the representational dissimilarity matrix (RDM)\n",
    "  Args:\n",
    "    resp (ndarray): S x N matrix with population responses to\n",
    "      each stimulus in each row\n",
    "  Returns:\n",
    "    ndarray: S x S representational dissimilarity matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # z-score responses to each stimulus\n",
    "  zresp = zscore(resp, axis=1)\n",
    "\n",
    "  # Compute RDM\n",
    "  RDM = 1 - (zresp @ zresp.T) / zresp.shape[1]\n",
    "\n",
    "  return RDM\n",
    "\n",
    "def visualize_image_and_filter(image, conv_filter):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "    axes[0].imshow(image, vmin = -5, vmax = 5, cmap = 'gray')\n",
    "    axes[0].set(xticks = [0.5, 1.5, 2.5],\n",
    "                yticks = [0.5, 1.5, 2.5],\n",
    "                xticklabels = '',\n",
    "                yticklabels = '',\n",
    "                title = 'Image')\n",
    "\n",
    "    for r in range(4):\n",
    "      for c in range(4):\n",
    "        axes[0].annotate(image[c, r], (r, c), color = 'r', fontsize = 20)\n",
    "    axes[0].grid(color = 'r')\n",
    "\n",
    "\n",
    "    axes[1].imshow(conv_filter, vmin = -5, vmax = 5, cmap = 'gray')\n",
    "    axes[1].set(xticks = [0.5, 1.5, 2.5],\n",
    "                yticks = [0.5, 1.5, 2.5],\n",
    "                xticklabels = '',\n",
    "                yticklabels = '',\n",
    "                title = 'Convolutional filter')\n",
    "\n",
    "    for r in range(3):\n",
    "      for c in range(3):\n",
    "        axes[1].annotate(conv_filter[c, r], (r, c), color = 'r', fontsize = 20)\n",
    "    axes[1].grid(color = 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgV5PPFRlejG"
   },
   "source": [
    "## Load data\n",
    "\n",
    "We will be analyzing neural data like that used in the [Yamins 2014 paper](https://www.pnas.org/content/111/23/8619) mentioned in class.\n",
    "\n",
    "The authors presented images to monkeys while recording from areas V4 and IT and compared the neural data to performance and activity of several artificial neural network models.\n",
    "\n",
    "**Images:** They presented images from 7 different categories (Animals, Cars, Chairs, Faces, Fruits, Planes, and Tables). In each category, they had 7 separate objects. They created 40 configurations of each object, showing it on top of different backgrounds and in different sizes, shapes, and rotations. They presented each of these 1960 images (7 categories x 7 objects x 40 examples/configurations) around 40 times.\n",
    "\n",
    "We won't load in the images as it will take too much memory, but you can [click here](https://drive.google.com/file/d/10PYDy_naIE88aVBEhrGsw8uIWpl-6uBs/view?usp=sharing) to view 5 examples for 4 of the 7 objects in the Animals category.\n",
    "\n",
    "**Neural data**: While presenting the above images, the authors recorded 168 cells from area IT and 128 cells from area V4.\n",
    "\n",
    "Execute the next cell to load in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3122,
     "status": "ok",
     "timestamp": 1766091516450,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "9V-uHL7glejH",
    "outputId": "d96d14bc-9aab-4a06-9810-0e77d3a2ef8b"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "IT_resps = np.load('IT_responses.npy')\n",
    "V4_resps = np.load('V4_responses.npy')\n",
    "\n",
    "categories = np.array(['Animals', 'Cars', 'Chairs', 'Faces', 'Fruits', 'Planes', 'Tables'])\n",
    "\n",
    "print(\"IT_resps:\" + str(IT_resps.shape))\n",
    "print(\"V4_resps:\" + str(V4_resps.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwKJcwGJlejI"
   },
   "source": [
    "<br>**Info about neural data:**\n",
    "<br><br>\n",
    "`IT_resps` is a numpy array of shape 7 x 7 x 40 x 168. The first dimension corresponds to 7 categories, the second to  7 objects per category. The third dimension is the 40 different examples/configurations. And the fourth dimension corresponds to the 168 area IT neurons.\n",
    "\n",
    "`V4_resps` is set up similarly, except there were 128 neurons recorded.\n",
    "\n",
    "The numbers inside each array convey the average response to each image for each neuron. The units are NOT firing rates, but normalized, baseline-adjusted spike counts. Note that any temporal information within spiking responses is ignored by this approach.\n",
    "\n",
    "If you're interested, here's the detailed description of the preprocessing procedure, from the [Supplemental Material](https://www.pnas.org/doi/10.1073/pnas.1403112111#supplementary-materials): <br>\n",
    ">_\"For each image repetition and electrode, scalar firing rates were obtained from spike trains by averaging spike counts in the period 70-170 ms after stimulus presentation, a measure of neural response that has recently been shown to match behavioral performance characteristics very closely. Background firing rate, defined as the mean within-block spike count for blank images, was subtracted from the raw response. Additionally, the signal was normalized such that its per-block variance is 1. Final neuron output responses were obtained for each image and site by averaging over image repetitions.\"_\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG7OSu9RlejI"
   },
   "source": [
    "# Part 1: Individual neural responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pM5A-KrwlejJ"
   },
   "source": [
    "### 2a: Thinking about selectivity\n",
    "\n",
    "Execute the following cell to visualize an example neuron's responses to the 1960 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1766091516893,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "WfC4CM6SlejJ",
    "outputId": "627e70a0-64cd-4c6e-cb15-771c13d488f6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "ax.plot(IT_resps[:, :, :, 1].reshape((-1,)), 'k')\n",
    "ylim = ax.get_ylim()\n",
    "for i in range(0, 1900, 7*40):\n",
    "  ax.plot([i, i], ylim, 'r')\n",
    "\n",
    "_ = ax.set(xticks = np.arange(7*40/2, 1960, 7*40),\n",
    "       xticklabels = categories,\n",
    "       ylabel = 'Neural Response');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lrtdynzlejJ"
   },
   "source": [
    "<br>**2a. Interpret the plot and evaluate this neuron's selectivity.**\n",
    "\n",
    "i) Does this neuron seem to be category selective? Why or why not?\n",
    "\n",
    "ii) Does this neuron seem to be strongly object selective? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIDG-dvMexJj"
   },
   "source": [
    "<font color=\"#2AAA8A\">**Answer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Selectivity index\n",
    "\n",
    "We will quantify the object selectivity of each neuron by using a selectivity index. First, we compute the mean activity per object from the neuron. Then we can compute the selectivity index as follows:\n",
    "\n",
    ">selectivity = $\\frac{\\mu_{max} - \\mu_{-max}}{\\mu_{max} + \\mu_{-max}}$\n",
    "\n",
    "where $\\mu_{max}$ is the highest object mean activity and $\\mu_{-max}$ (read as mu _not_ max) is the mean activity across all the other objects. This gives us how much higher the mean activity for the preferred object is than for the rest of the objects, normalized by that neuron's overall amount of activity.\n",
    "\n",
    "i) If the neuron had the same mean activity for all objects, what would this selectivity index equal?\n",
    "\n",
    "ii) If the neuron only responsed to one object, what would this selectivity index equal?\n",
    "\n",
    "iii) Would you expect V4 neurons to have lower or higher selectivity indices than area IT neurons, on average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIDG-dvMexJj"
   },
   "source": [
    "<font color=\"#2AAA8A\">**Answer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U35dBzJlejL"
   },
   "source": [
    "### 2c: Quantifying object selectivity (coding)\n",
    "\n",
    "\n",
    "Compute this selectivity index for all of the IT neurons (stored in the array `selectivity_indices_IT` and for V4 in `selectivity_indices_V4`. Both of these arrays should be of shape (N neurons,). \n",
    "\n",
    "Hint: You can code this with a for-loop or with vectorized operations (the latter is typically more efficient, but the former may be more intuitive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1766091517703,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "y01OVldWlejL",
    "outputId": "fbedc798-6b56-4488-edc7-8acb3e2c763e"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute selectivity indices\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "print(\"The median SI for IT is {:.3f} and the median SI for V4 is {:.3f}\\n\".format(\n",
    "    np.median(selectivity_indices_IT), \n",
    "    np.median(selectivity_indices_V4)))\n",
    "\n",
    "# Visualize results with histograms\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "_, bins, _ = axes[1].hist(selectivity_indices_IT, 100, color='#3c649f');\n",
    "axes[1].set(title='Selectivity indices for cells in IT', xlabel='Selectivity Index', ylabel='Count')\n",
    "_ = axes[0].hist(selectivity_indices_V4, bins, color='#2c456b');\n",
    "axes[0].set(title='Selectivity indices for cells in V4', ylabel='Count')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdUbIBc-lejR"
   },
   "source": [
    "### 2d: Evaluate selectivity indices in V4 and IT populations\n",
    "\n",
    "i) What are your conclusions about area V4 and IT object selectivity based on these histograms?\n",
    "\n",
    "ii) Do these results support the idea of area IT containing \"grandmother cells\", or single object detectors? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpAPScgAlejR"
   },
   "source": [
    "<font color=\"#2AAA8A\"><span style=\"font-size:larger;\">\n",
    "**Answer**<br>\n",
    "<br>\n",
    "i) \n",
    "<br><br>\n",
    "ii) \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FviczmC8KXFB"
   },
   "source": [
    "### 2e: Quantitative comparison of SI distributions\n",
    "\n",
    "Write code to run a statistical test to quantitatively compare the selectivities in V4 and IT. Are the distributions significantly different?\n",
    "\n",
    "Choose an appropriate statistical test! This might help: https://docs.scipy.org/doc/scipy/reference/stats.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766091517709,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "BAl9FY2gii2x",
    "outputId": "4045ae98-2bba-4ba4-94bc-124f593c1a8d"
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbe_LVA7lejS"
   },
   "source": [
    "# Part 2: Population representations via RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etzP2GSUlejS"
   },
   "source": [
    "### 2f: RDM for area IT (coding)\n",
    "\n",
    "Let's look at how the different objects and categories are represented in area IT by constructing a representational dissimilarity matrix (RDM).\n",
    "\n",
    "**Step 1)** We will compute the dissimilarity using the mean response per object, after averaging over configurations. Thus the first step is to average the neural responses over the 40 different configurations of each object.\n",
    "\n",
    "**Step 2)** Compute the representational dissimilarity matrix for this data and store it in a variable called `rdm`. This should be a 49 x 49 matrix where each entry equals 1 minus the correlation of the neural responses to two images (the image represented by the row number and the image represented by the column number). So the entry at row 4, column 8 is 1 minus the correlation of the responses to images 4 and 8.\n",
    "\n",
    "Try to vectorize this code! If you want, you can compute it in a loop first and then use that to check your vectorized answer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1766091518127,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "jyHUeZDzlejS",
    "outputId": "52344f0b-5225-4838-a42f-d6d8ed2329e8"
   },
   "outputs": [],
   "source": [
    "# Compute RDM for IT responses\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Step 1\n",
    "\n",
    "\n",
    "# Step 2\n",
    "\n",
    "\n",
    "\n",
    "# Visualize rdm (pre-defined in helper code at top)\n",
    "visualize_rdm(rdm);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apU0vyRalejS"
   },
   "source": [
    "### 2g: Interpreting RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nZKXCtPlejS"
   },
   "source": [
    "**Answer the following questions. RDMs for both the V4 neural responses and the IT neural responses are shown in the cell below.**\n",
    "\n",
    "i) What does the number in a given row and column of the RDM represent? Be specific about what the neural responses are, what we've averaged over, etc.\n",
    "\n",
    "ii) Within a category, does the population of IT neurons often respond similarly for different objects? What category is this especially true of? What does this tell you about the type of coding in area IT, e.g. the complexity of the features encoded?\n",
    "\n",
    "iii) Complete this sentence by filling in the appropriate object categories. \"We can predict that a simple linear decoder trained on IT activity would be able to discriminate between [these categories] with high accuracy, while decoding accuracy between [these categories] would be much lower.\"\n",
    "\n",
    "iv) How does the V4 RDM differ from the IT RDM? What does this tell you about V4 processing as compared to IT processing?\n",
    "\n",
    "v) If you recorded from V1 neurons and constructed an RDM, what would you expect it to look like?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1766093202396,
     "user": {
      "displayName": "Kristina Penikis",
      "userId": "11146911841924357157"
     },
     "user_tz": 300
    },
    "id": "DXdH6Q6rlejT",
    "outputId": "0ffe78d2-1f8f-4a88-b0d1-139790fba45e"
   },
   "outputs": [],
   "source": [
    "V4_rdm = compute_RDM(np.mean(V4_resps, axis = 2).reshape((49, -1)))\n",
    "IT_rdm = compute_RDM(np.mean(IT_resps, axis = 2).reshape((49, -1)))\n",
    "\n",
    "# Visualize RDMs\n",
    "fig, (ax, ax2) = plt.subplots(ncols=2,figsize=(9,4),\n",
    "                  gridspec_kw={\"width_ratios\":[1,1]})\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "im = visualize_rdm(V4_rdm, ax=ax, title='V4')\n",
    "visualize_rdm(IT_rdm, ax=ax2, title='IT')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsqUwvE0lejT"
   },
   "source": [
    "<font color=\"#2AAA8A\"><span style=\"font-size:larger;\">**Answer**<br>\n",
    "<br>\n",
    "i) \n",
    "<br><br>\n",
    "ii) \n",
    "<br><br>\n",
    "iii) \n",
    "<br><br>\n",
    "iv) \n",
    "<br><br>\n",
    "v) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
